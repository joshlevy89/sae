{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8747e1-de06-4455-b9c7-5b05b90e19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcd14cc-e174-4692-8f61-7327239472ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448e3e76-59f8-4e98-822e-59941dd939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('ola13/small-the_pile-dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a57e6b0-d88b-434b-958f-3adaadcd76cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc8f109-41ea-4eae-a47d-ad5df90b129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/cache/' # change or comment out as desired \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name_or_path, revision, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path, device_map=device, revision=revision, trust_remote_code=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0\n",
    "    return model, tokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "revision = 'gptq-4bit-32g-actorder_True'\n",
    "user_tag = \"[INST] \"\n",
    "assistant_tag = \" [/INST]\"\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path, revision, device)\n",
    "\n",
    "model_mistral=model\n",
    "tokenizer_mistral=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b2b912-a8a9-4417-a0fd-37ec52b5e79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2196ba296ed4e27af94576c3be350aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28deb8a5b7c42e6b7eaba80bb209e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907d7f63833d4ce2b9f7dc443cf1b848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec06cdf222479b9d25ee3bc06daceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c1aad0305f421fa8b98232243b12c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "tokenizer_pythia = tokenizer\n",
    "model_pythia = model\n",
    "model_pythia.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdef7109-c292-4ef8-a18d-fbef80b2d60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4237557a6940fcaa3d9be0d18ac1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83385dc62af84faa823532958a45096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/825M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f520e3b328b84b9ba0d45b68052e69bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d689f37175f740dda23fd38d3ff0c221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7bab0ad4144e1ca961332d30ed030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a094e07609a043eba3dbe17363f4626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "device = 'cuda:0'\n",
    "model_name_or_path = 'TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ'\n",
    "revision = 'gptq-4bit-32g-actorder_True'\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path, revision, device)\n",
    "\n",
    "model_tiny=model\n",
    "tokenizer_tiny=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c82027-f99f-46cf-b0bc-b03821e85cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3855ebb392714c7986fcb628064c488f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/962 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ced5a322334150ba8e6552ed2b8e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe92d628f0149c182398c42bac50335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ac407ee3054c6496c36ff27ce88368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/962 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bf9330e8524927aa67645afa20fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f3948d1ec74d6eaf95db95b6586a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352812b02d0b4f5dba0321416178dd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3efd2ba8a54ebc9fcebbe24456c1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "revision = 'gptq-4bit-32g-actorder_True'\n",
    "model_name_or_path = 'TheBloke/Mistral-7B-v0.1-GPTQ'\n",
    "\n",
    "model, tokenizer = load_model(model_name_or_path, revision, device)\n",
    "\n",
    "model_mistralpre = model\n",
    "tokenizer_mistralpre=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6119c2a-a76c-4793-b9a9-b130d7018b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [01:01<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# get activations\n",
    "\n",
    "device = 'cuda:0'\n",
    "# model = model_pythia.to(device)\n",
    "# model = model_mistral.to(device)\n",
    "# model = model_tiny.to(device)\n",
    "# tokenizer = tokenizer_tiny\n",
    "model = model_mistralpre\n",
    "tokenizer = tokenizer_mistralpre\n",
    "\n",
    "\n",
    "bs = 16\n",
    "layer = 3\n",
    "context_len = 128\n",
    "all_acts = []\n",
    "for i in tqdm(range(1024, 2048, bs)):\n",
    "    text = data['train']['text'][i:i+bs]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    inputs_mod = {'input_ids': inputs['input_ids'][:, :context_len], 'attention_mask': inputs['attention_mask'][:, :context_len]}\n",
    "    output = model(**inputs_mod, output_hidden_states=True)\n",
    "    acts = output['hidden_states'][layer+1].detach().reshape(bs*context_len, -1)\n",
    "    all_acts.extend(acts)\n",
    "all_acts = torch.stack(all_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f619f-9fe1-4a0e-aa6e-7738568ab0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

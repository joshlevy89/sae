{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f36b757-4a6c-4541-a740-3b1617a39494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('ola13/small-the_pile-dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b2fedd-03ea-47b7-a400-14348e41216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e96065-34bb-4b9d-aaa6-e1e8d14195c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13276"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7342ca-b369-4594-83ca-af4d1945458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, I am looking for a way to get my name in the mail. I am looking for'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load llm\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs)\n",
    "tokenizer.decode(tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "320b6d81-a718-4d4c-a035-9a7ca66eebfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fe4db44-71c9-40f2-9275-5fd0a1b0a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe597e5f-17eb-4228-88dc-5d00cb91e064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cce87629-b87a-41db-9210-1f9598fcff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sae \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, model_dim, sae_dim):\n",
    "        super(SAE, self).__init__()\n",
    "        self.encoder = nn.Linear(model_dim, sae_dim, bias=True)\n",
    "        self.decoder = nn.Linear(sae_dim, model_dim, bias=True)\n",
    "        self.model_bias = nn.Parameter(torch.randn(model_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.relu(self.encoder(x - self.model_bias))\n",
    "        xhat = self.decoder(f)\n",
    "        return xhat\n",
    "\n",
    "model_dim = 512\n",
    "sae_dim = model_dim * 4\n",
    "sae = SAE(model_dim, sae_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4874322d-d9b6-488f-9979-ed4408bd7c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "y = sae(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7475bf1e-9c56-4fb7-8d75-5b8f94873ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trained  0\n",
      "cache  0\n",
      "0\n",
      "cache  128\n",
      "1\n",
      "cache  256\n",
      "2\n",
      "cache  384\n",
      "3\n",
      "cache  464\n",
      "4\n",
      "cache  592\n",
      "5\n",
      "cache  720\n",
      "6\n",
      "cache  848\n",
      "7\n",
      "cache  976\n",
      "8\n",
      "cache  1104\n",
      "9\n",
      "cache  1232\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# get the activations, train sae\n",
    "\n",
    "layer = 3\n",
    "context_len = 128\n",
    "n_train_tokens = context_len * 20\n",
    "n_trained = 0\n",
    "max_cache_size = context_len * 10\n",
    "cache = torch.empty(0, model_dim)\n",
    "context_idx = 0  # TODO: need to randomize documents? \n",
    "sae_batch_size = context_len * 1\n",
    "\n",
    "# sample from the cache\n",
    "def sample_and_remove(cache, n):\n",
    "    indices = torch.randperm(cache.size(0))[:n]\n",
    "    sampled_acts = cache[indices]\n",
    "    mask = torch.ones(cache.size(0), dtype=torch.bool)\n",
    "    mask[indices] = False\n",
    "    new_cache = cache[mask]\n",
    "    return sampled_acts, new_cache\n",
    "\n",
    "# until trained on X tokens:\n",
    "while n_trained < n_train_tokens:\n",
    "    # if the cache is at least half empty:\n",
    "    if cache.shape[0] < max_cache_size // 2:\n",
    "        while cache.shape[0] < max_cache_size:\n",
    "            # TODO: change this to batches\n",
    "            print(context_idx)\n",
    "            text = data['train']['text'][context_idx]\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "            inputs_mod = {'input_ids': inputs['input_ids'][0, :context_len].view(1, -1), 'attention_mask': inputs['attention_mask'][0, :context_len].view(1, -1)}\n",
    "            output = model(**inputs_mod, output_hidden_states=True)\n",
    "            acts = output['hidden_states'][layer+1][0,:, :]\n",
    "            cache = torch.cat((cache, acts))\n",
    "            context_idx += 1\n",
    "\n",
    "    # sample from the cache\n",
    "    sae_acts_batch, cache = sample_and_remove(cache, sae_batch_size)\n",
    "\n",
    "    # train another step in the autoencoder\n",
    "    \n",
    "\n",
    "    # increment trained\n",
    "    n_trained += sae_batch_size\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
